{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import re\n",
    "from textblob import TextBlob\n",
    "import csv\n",
    "import nltk\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import os\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_curve, auc\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from nltk.tokenize import word_tokenize, WhitespaceTokenizer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>House Dem Aide: We Didn’t Even See Comey’s Let...</td>\n",
       "      <td>Darrell Lucus</td>\n",
       "      <td>House Dem Aide: We Didn’t Even See Comey’s Let...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>FLYNN: Hillary Clinton, Big Woman on Campus - ...</td>\n",
       "      <td>Daniel J. Flynn</td>\n",
       "      <td>Ever get the feeling your life circles the rou...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Why the Truth Might Get You Fired</td>\n",
       "      <td>Consortiumnews.com</td>\n",
       "      <td>Why the Truth Might Get You Fired October 29, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>15 Civilians Killed In Single US Airstrike Hav...</td>\n",
       "      <td>Jessica Purkiss</td>\n",
       "      <td>Videos 15 Civilians Killed In Single US Airstr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Iranian woman jailed for fictional unpublished...</td>\n",
       "      <td>Howard Portnoy</td>\n",
       "      <td>Print \\nAn Iranian woman has been sentenced to...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>95</td>\n",
       "      <td>White House Confirms More Gitmo Transfers Befo...</td>\n",
       "      <td>Edwin Mora</td>\n",
       "      <td>President Barack Obama will likely release mor...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>96</td>\n",
       "      <td>The Geometry of Energy and Meditation of Buddha</td>\n",
       "      <td>NaN</td>\n",
       "      <td>License DMCA \\nA mandala is a visual symbol of...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>97</td>\n",
       "      <td>Poll: Most Voters Have Not Heard of Democratic...</td>\n",
       "      <td>Katherine Rodriguez</td>\n",
       "      <td>There is a minefield of potential 2020 electio...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>98</td>\n",
       "      <td>Migrants Confront Judgment Day Over Old Deport...</td>\n",
       "      <td>Vivian Yee</td>\n",
       "      <td>There are a little more than two weeks between...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>99</td>\n",
       "      <td>M.I.T., N.Y.U. and Yale Are Sued Over Retireme...</td>\n",
       "      <td>Tara Siegel Bernard</td>\n",
       "      <td>Three prominent universities were sued on Tues...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    id                                              title  \\\n",
       "0    0  House Dem Aide: We Didn’t Even See Comey’s Let...   \n",
       "1    1  FLYNN: Hillary Clinton, Big Woman on Campus - ...   \n",
       "2    2                  Why the Truth Might Get You Fired   \n",
       "3    3  15 Civilians Killed In Single US Airstrike Hav...   \n",
       "4    4  Iranian woman jailed for fictional unpublished...   \n",
       "..  ..                                                ...   \n",
       "95  95  White House Confirms More Gitmo Transfers Befo...   \n",
       "96  96    The Geometry of Energy and Meditation of Buddha   \n",
       "97  97  Poll: Most Voters Have Not Heard of Democratic...   \n",
       "98  98  Migrants Confront Judgment Day Over Old Deport...   \n",
       "99  99  M.I.T., N.Y.U. and Yale Are Sued Over Retireme...   \n",
       "\n",
       "                 author                                               text  \\\n",
       "0         Darrell Lucus  House Dem Aide: We Didn’t Even See Comey’s Let...   \n",
       "1       Daniel J. Flynn  Ever get the feeling your life circles the rou...   \n",
       "2    Consortiumnews.com  Why the Truth Might Get You Fired October 29, ...   \n",
       "3       Jessica Purkiss  Videos 15 Civilians Killed In Single US Airstr...   \n",
       "4        Howard Portnoy  Print \\nAn Iranian woman has been sentenced to...   \n",
       "..                  ...                                                ...   \n",
       "95           Edwin Mora  President Barack Obama will likely release mor...   \n",
       "96                  NaN  License DMCA \\nA mandala is a visual symbol of...   \n",
       "97  Katherine Rodriguez  There is a minefield of potential 2020 electio...   \n",
       "98           Vivian Yee  There are a little more than two weeks between...   \n",
       "99  Tara Siegel Bernard  Three prominent universities were sued on Tues...   \n",
       "\n",
       "    label  \n",
       "0       1  \n",
       "1       0  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  \n",
       "..    ...  \n",
       "95      0  \n",
       "96      1  \n",
       "97      0  \n",
       "98      0  \n",
       "99      0  \n",
       "\n",
       "[100 rows x 5 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def explore_data(dataset):\n",
    "\tdf = pd.read_csv(os.path.join(dataset))\n",
    "\treturn df \n",
    "#data is taken from lair dataset\n",
    "train_news = explore_data('train.csv')\n",
    "train_news.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df1 = train_news[['text', 'label']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Getting rid of empty lines\n",
    "df1 = df1[df1.text.isna() == False]\n",
    "length_df1 = len(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Build sublist of original df1, contains # lines picked at random, out of 20671 possible\n",
    "random_indexes = list(np.random.choice(length_df1 - 2, 3000, replace=False))\n",
    "df1 = df1.iloc[random_indexes]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function dissects text i, attributes polarity scores, positive/negative/neutral, polarity or not, and subject\n",
    "def sentiment_analyzer(dataframe):\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    scores = [sid.polarity_scores(i) for i in dataframe.text]\n",
    "    compounds = np.array([i['compound'] for i in scores], dtype='float32')\n",
    "    abs_compounds = np.array([np.sqrt(i ** 2) for i in compounds], dtype='float32')\n",
    "    negs = np.array([i['neg'] for i in scores], dtype='float32')\n",
    "    poss = np.array([i['pos'] for i in scores], dtype='float32')\n",
    "    neus = np.array([i['neu'] for i in scores], dtype='float32')\n",
    "    sent = dataframe['text'].apply(lambda x: TextBlob(x).sentiment)\n",
    "    pol = np.array([s[0] for s in sent], dtype='float32')\n",
    "    abs_pol = np.array([np.sqrt(i ** 2) for i in pol], dtype='float32')\n",
    "    subj = np.array([s[1] for s in sent], dtype='float32')\n",
    "\n",
    "    return compounds, abs_compounds, negs, poss, neus, sent, pol, abs_pol, subj\n",
    "\n",
    "\n",
    "compounds, abs_compounds, negs, poss, neus, sent, pol, abs_pol, subj = sentiment_analyzer(df1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Adding columns to df1, matching them with newly created variables\n",
    "df1['compounds'] = compounds\n",
    "df1['abs_compounds'] = abs_compounds\n",
    "df1['negs'] = negs\n",
    "df1['neus'] = neus\n",
    "df1['poss'] = poss\n",
    "df1['pol'] = pol\n",
    "df1['abs_pol'] = abs_pol\n",
    "df1['subj'] = subj\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = df1[['compounds', 'negs', 'neus', 'poss', 'pol', 'subj']]\n",
    "y = df1['label']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# First classifier\n",
    "lrxtrain, lrxtest, lrytrain, lrytest = train_test_split(X, y)\n",
    "lr = LogisticRegression()\n",
    "lr.fit(lrxtrain, lrytrain)\n",
    "lrpreds = lr.predict(lrxtest)\n",
    "accuracy = accuracy_score(lrytest, lrpreds)\n",
    "f1 = f1_score(lrytest, lrpreds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5306666666666666 0.5056179775280899\n"
     ]
    }
   ],
   "source": [
    "print(accuracy, f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x_values = df1[['text', 'compounds', 'abs_compounds', 'negs', 'neus', 'poss', 'pol', 'abs_pol', 'subj']]\n",
    "y_values = df1['label']\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(x_values, y_values)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Cleans article from numbers, capital letters, punctuation and spaces for better classifier results\n",
    "def clean_article(article):\n",
    "    art = re.sub(\"[^A-Za-z0-9' ]\", '', str(article))\n",
    "    art2 = re.sub(\"[( ' )(' )( ')]\", ' ', str(art))\n",
    "    art3 = re.sub(\"\\s[A-Za-z]\\s\", ' ', str(art2))\n",
    "    return art3.lower()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### Tokenize\n",
    "# Stop_words will ignore common english words which are noise (the / a / an / etc.)\n",
    "# Max_df / min_df : ignore words which frequencies are above/under those thresholds\n",
    "\n",
    "bow = CountVectorizer(stop_words='english', ngram_range=(1, 2), max_features=998, max_df=1.0, min_df=1, binary=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "training_data = bow.fit_transform(xtrain.text)\n",
    "test_data = bow.transform(xtest.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dftrain = pd.DataFrame(training_data.toarray())\n",
    "dftrain.columns = bow.get_feature_names()\n",
    "# dftrain = dftrain.drop('s', axis=1)\n",
    "# dftrain = dftrain.drop('m', axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dftest = pd.DataFrame(test_data.toarray())\n",
    "dftest.columns = bow.get_feature_names()\n",
    "# dftest = dftest.drop('s', axis=1)\n",
    "# dftest = dftest.drop('m', axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.928 0.926027397260274\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mohdf\\anaconda3\\envs\\ddp\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "### Set up 2nd classifier\n",
    "lr2 = LogisticRegression()\n",
    "lr2.fit(dftrain, ytrain)\n",
    "lr2_preds = lr2.predict(dftest)\n",
    "accuracy = accuracy_score(ytest, lr2_preds)\n",
    "f1 = f1_score(ytest, lr2_preds)\n",
    "\n",
    "print(accuracy, f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(lr2, open(\"model.pkl\", \"wb\"), protocol=2)\n",
    "pickle.dump(clean_article, open(\"clean_article.pkl\", 'wb'))\n",
    "pickle.dump(bow, open(\"bow2.pkl\", 'wb'), protocol=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
